---
title: Initiation au Fablab
description: Un guide pour d√©couvrir les bases du Fablab et de la fabrication num√©rique.
authors: [yann_v]
tags: [fablab, ai, 3d-printing, robotics]
hide_table_of_contents: false
slug: welcome-fablabs
---

# Workshop : D√©couverte des Fablabs, de l'Impression 3D et de l'IA Embarqu√©e

Bienvenue dans ce workshop immersif o√π vous allez explorer les Fablabs, l'impression 3D et l'IA embarqu√©e. Pr√©parez-vous √† plonger dans un univers de cr√©ativit√©, d'innovation et d'intelligence artificielle !

## Introduction aux Fablabs et √† l'Impression 3D

### Qu'est-ce qu'un Fablab ?

Un **Fablab**, ou "Fabrication Laboratory", est un espace collaboratif o√π chacun peut venir cr√©er, apprendre et partager. Initi√© par le **MIT** dans les ann√©es 2000, ce concept permet d'acc√©der √† des outils de fabrication num√©rique comme des imprimantes 3D, des d√©coupeuses laser, et bien plus encore. Les Fablabs sont des lieux o√π les id√©es prennent vie gr√¢ce √† la technologie et √† l'innovation.

<!-- truncate -->

### Les machines du Fablab

Notre Fablab est √©quip√© de diverses machines pour r√©pondre √† tous vos besoins cr√©atifs :

- **Imprimantes 3D** : Pour fabriquer des prototypes et des pi√®ces en plastique.
- **D√©coupeuse laser** : Pour d√©couper et graver du bois, du plastique et du tissu.
- **Fraiseuse num√©rique** : Pour usiner des pi√®ces en bois, m√©tal ou plastique avec pr√©cision.
- **Espace √©lectronique** : √âquip√© pour cr√©er des circuits et des projets √©lectroniques.

### Les r√©alisations du Fablab

D√©couvrez quelques projets r√©alis√©s dans notre Fablab :

- **Tests de r√©sistance** : Impression de divers objets pour tester la qualit√© des mat√©riaux.
- **Am√©lioration de l'espace** : Cr√©ation de solutions de rangement et d'organisateurs de tiroirs.
- **Troph√©es personnalis√©s** : Fabrication de troph√©es pour une comp√©tition.
- **Cartes √©lectroniques** : D√©veloppement et assemblage de cartes √©lectroniques pour des projets internes.

## Plongeons dans l'Impression 3D

### Les filaments pour l'impression 3D

![Filaments](/assets/docs/filaments/filament-1.png)

Voici un tableau r√©capitulatif des diff√©rents types de filaments utilis√©s en impression 3D :

| Mat√©riau             | Type      | Facile √† Imprimer | R√©sistant | Durable | N√©cessite Enceinte | Flexible | R√©sistant aux UV |
| -------------------- | --------- | :---------------: | :-------: | :-----: | :----------------: | :------: | :--------------: |
| **PLA**              | Standard  |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚úîÔ∏è    |         ‚ùå         |    ‚ùå    |        ‚ùå        |
| **PETG**             | Standard  |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚úîÔ∏è    |         ‚úîÔ∏è         |    ‚ùå    |        ‚ùå        |
| **ABS**              | Standard  |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚úîÔ∏è    |         ‚úîÔ∏è         |    ‚ùå    |        ‚ùå        |
| **Flex**             | Technique |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚úîÔ∏è    |         ‚ùå         |    ‚úîÔ∏è    |        ‚ùå        |
| **Nylon**            | Technique |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚úîÔ∏è    |         ‚úîÔ∏è         |    ‚ùå    |        ‚ùå        |
| **ASA**              | Technique |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚úîÔ∏è    |         ‚úîÔ∏è         |    ‚ùå    |        ‚úîÔ∏è        |
| **Polycarbonate**    | Technique |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚úîÔ∏è    |         ‚úîÔ∏è         |    ‚ùå    |        ‚ùå        |
| **Fibre de Carbone** | Composite |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚ùå    |         ‚ùå         |    ‚ùå    |        ‚ùå        |
| **M√©tal**            | Composite |        ‚úîÔ∏è         |    ‚úîÔ∏è     |   ‚ùå    |         ‚ùå         |    ‚ùå    |        ‚ùå        |
| **Bois**             | Composite |        ‚úîÔ∏è         |    ‚ùå     |   ‚ùå    |         ‚ùå         |    ‚ùå    |        ‚ùå        |

### Le syst√®me AMS de Bambulab

Le syst√®me **AMS** de Bambulab permet l'impression 3D avec jusqu'√† quatre filaments diff√©rents sans changement manuel des bobines. Il utilise la reconnaissance automatique des filaments (**RFID**) et le changement automatique des bobines (**ABC**). L'AMS est √©galement √©quip√© d'un syst√®me de s√©chage des filaments pour am√©liorer la qualit√© d'impression.

![AMS](/assets/docs/bambulab/bambulab-2.png)

### L'extrudeur

L'**extrudeur** tire le filament et le fait avancer dans le corps chauffant. L'extrudeur de l'imprimante Bambulab X1 Carbon est un extrudeur direct, situ√© au-dessus de la plaque d'impression, offrant un meilleur contr√¥le de la temp√©rature et de la pression du filament.

![Extrudeur](/assets/docs/bambulab/bambulab-3.png)

### Le corps chauffant

Le filament entre dans le corps chauffant, o√π il est fondu et extrud√© √† travers la buse. Le corps chauffant est √©quip√© d'une cartouche chauffante et d'un capteur de temp√©rature pour contr√¥ler la temp√©rature du filament.

![Corps chauffant](/assets/docs/bambulab/bambulab-4.png)

### La plaque

La **plaque** est la surface sur laquelle l'impression est r√©alis√©e. Elle est chauff√©e pour am√©liorer l'adh√©rence du filament et peut √™tre textur√©e ou √©quip√©e de surfaces amovibles pour faciliter le retrait de l'impression.

![Plaque](/assets/docs/bambulab/bambulab-5.png)

### Le logiciel de d√©coupe

Un **slicer** convertit votre mod√®le 3D en instructions d'impression. Dans ce guide, nous utiliserons **OrcaSlicer**, un slicer avanc√© compatible avec la plupart des imprimantes 3D.

![Logiciel](/assets/docs/orca/orcaslicer-1.png)

#### Importation de votre mod√®le 3D

1. T√©l√©chargez un mod√®le 3D depuis des plateformes comme [MakerWorld](https://makerworld.com/), [Printables](https://www.printables.com/), [Thingiverse](https://www.thingiverse.com/), [Cults](https://cults3d.com/), ou [MyMiniFactory](https://www.myminifactory.com/).
2. Importez le mod√®le dans OrcaSlicer en cliquant sur "Importer".

![Importation](/assets/docs/orca/orcaslicer-2.png)

#### Cr√©ation du contexte d'impression

1. Configurez l'imprimante dans le logiciel (mod√®le, diam√®tre de la buse, taille du plateau, mat√©riau).
2. Choisissez le type de plateau (lisse, textur√©, technique).
3. S√©lectionnez le filament pour chaque fichier.
4. D√©finissez la hauteur des couches pour ajuster la qualit√© de l'impression.

![Contexte](/assets/docs/orca/orcaslicer-3.png)

#### Positionnement de l'objet sur le plateau

Choisissez l'orientation de l'objet pour des raisons techniques et esth√©tiques. L'impression par d√©p√¥t de filament (FDM) laisse des traces visibles, alors choisissez judicieusement l'orientation.

![Couches](/assets/docs/orca/orcaslicer-4.png)

#### D√©finition du remplissage de la pi√®ce

1. Ajustez la densit√© de remplissage pour contr√¥ler la solidit√© et le poids de l'impression.
2. Choisissez parmi diff√©rents motifs de remplissage (nids d'abeille, gyro√Ødes, grilles).

![Remplissage](/assets/docs/orca/orcaslicer-5.png)

#### Cr√©ation de supports

Les supports sont n√©cessaires pour imprimer des parties en surplomb. Activez les supports et choisissez entre les types g√©om√©triques et organiques.

![Supports](/assets/docs/orca/orcaslicer-6.png)

#### Param√®tres avanc√©s

OrcaSlicer offre des param√®tres avanc√©s pour affiner votre impression :

- **Vitesse d'impression** : Contr√¥lez la vitesse de l'extrudeuse.
- **R√©glages de temp√©rature** : D√©finissez les temp√©ratures de la buse et du plateau.
- **Vitesse du ventilateur** : Ajustez la vitesse de refroidissement du filament.
- **R√©glages de r√©traction** : G√©rez la r√©traction du filament pour √©viter les bavures.

#### Sauvegarde et exportation des fichiers d√©coup√©s

1. Sauvegardez le fichier d√©coup√©.
2. Exportez-le vers l'imprimante en ins√©rant la carte SD.

![Exportation](/assets/docs/orca/orcaslicer-7.png)

#### Pr√©paration de l'imprimante

1. V√©rifiez que le filament est correctement charg√©.
2. Assurez-vous qu'il y a suffisamment de filament pour l'impression.
3. Nettoyez et nivelez la plaque.
4. Lancez l'impression.

## Introduction √† l'IA Embarqu√©e

### Qu'est-ce que l'embarqu√© ?

L'embarqu√© d√©signe les **syst√®mes informatiques int√©gr√©s** √† des objets du quotidien, comme les montres connect√©es ou les robots autonomes. Ces syst√®mes sont optimis√©s pour ex√©cuter des t√¢ches sp√©cifiques avec des ressources limit√©es.

### Pourquoi l'embarqu√© est-il crucial aujourd'hui ?

- **Efficacit√© √©nerg√©tique** : Fonctionne avec une faible consommation d'√©nergie.
- **Ind√©pendance et autonomie** : Fonctionne 100% offline.
- **Temps r√©el** : R√©ponses ultra-rapides pour des applications critiques.

### Diff√©rence entre un microcontr√¥leur et un micro-ordinateur

| Caract√©ristique              | Microcontr√¥leur ‚öôÔ∏è                                                               | Micro-ordinateur üíª                                                                        |
| ---------------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| **D√©finition**               | Un circuit int√©gr√© avec processeur, m√©moire et p√©riph√©riques sur une seule puce. | Un petit ordinateur avec processeur, m√©moire, syst√®me d'exploitation et ports d'extension. |
| **Syst√®me d'exploitation**   | Aucun (ex√©cute directement un programme en boucle)                               | Oui (ex. Linux sur Raspberry Pi)                                                           |
| **Puissance de calcul**      | Faible, optimis√© pour des t√¢ches simples et r√©p√©titives                          | Plus puissant, capable d'ex√©cuter des applications complexes                               |
| **M√©moire (RAM & Stockage)** | Tr√®s limit√©e (quelques Ko √† Mo)                                                  | Plus importante (512 Mo √† plusieurs Go)                                                    |
| **Consommation d'√©nergie**   | Tr√®s basse (fonctionne sur batterie longtemps)                                   | Plus √©lev√©e (n√©cessite souvent une alimentation)                                           |
| **Programmation**            | Code souvent √©crit en C ou en assembleur, avec un temps d'ex√©cution pr√©cis       | Peut ex√©cuter des langages vari√©s (Python, Java, C++) et plusieurs processus en parall√®le  |
| **Exemples d'utilisation**   | Commande d'un moteur, gestion de capteurs, robotique basique                     | Vision par ordinateur, IA embarqu√©e, serveurs l√©gers, stations m√©t√©o connect√©es            |

### Perspectives futures

Avec l'√©volution des microcontr√¥leurs, on voit appara√Ætre des applications de plus en plus intelligentes :

- Voitures autonomes
- Monitoring m√©dical en temps r√©el
- Surveillance assist√©e par IA

## Quels types d'appareils pour l'embarqu√© ?

### Les plateformes les plus connues

| Type de Carte     | Caract√©ristiques                                   | Exemples                      |
| ----------------- | -------------------------------------------------- | ----------------------------- |
| **CPU Only**      | Peu gourmand en √©nergie, adapt√© aux t√¢ches l√©g√®res | Raspberry Pi, Arduino         |
| **CPU + GPU/TPU** | Acc√©l√©ration du calcul pour l'IA et la vision      | NVIDIA Jetson, Coral Edge TPU |
| **FPGA**          | Ultra sp√©cialis√©, faible consommation              | Xilinx, Lattice               |

### Comparatif entre Raspberry Pi et Jetson

| Crit√®re     | Raspberry Pi 5            | NVIDIA Jetson Nano        |
| ----------- | ------------------------- | ------------------------- |
| CPU         | Quad-core 64-bit          | Quad-core ARM Cortex-A57  |
| GPU         | Aucun d√©di√©               | 128 c≈ìurs CUDA            |
| M√©moire     | 8GB LPDDR4X               | 4GB LPDDR4                |
| Usage id√©al | IoT, automates, domotique | Vision par ordinateur, IA |

**Conclusion** : La Raspberry Pi est id√©ale pour des solutions simples et efficaces, tandis que le Jetson Nano est plus adapt√© pour des besoins IA avanc√©s.

### Focus sur la Raspberry Pi

![multiproc architecture](/assets/blog/welcome/rpi_schema.png)

La Raspberry Pi est un micro-ordinateur monocarte bas√© sur une architecture ARM, d√©velopp√© par la fondation Raspberry Pi. Con√ßue pour l'√©ducation, elle est devenue une r√©f√©rence dans le domaine du DIY, de l'embarqu√© et de l'IA.

- **Syst√®me d'exploitation** : Raspberry Pi OS, bas√© sur Debian.
- **Langages de programmation** : Python, C, Java, et plus.
- **Applications** : Domotique, robotique, serveurs web, stations m√©t√©o, IA embarqu√©e.

L'installation d'une Raspberry Pi est simple :

1. Une carte micro SD.
2. [RPI Imager](https://www.raspberrypi.com/software/) pour flasher la carte micro SD avec une image de votre choix.

## Le Computer Vision

![alt text](/assets/blog/welcome/computer_vision.png)

La Computer Vision est un domaine de l'IA qui permet aux machines de comprendre et d'analyser des images ou des vid√©os.

### You Only Look Once (YOLO)

![alt text](/assets/blog/welcome/yolo_map.png)

YOLO est un algorithme de d√©tection d'objets en temps r√©el qui traite l'image en une seule passe, rendant le processus extr√™mement rapide et efficace.

![alt text](/assets/blog/welcome/yolo_grid.png)

## L'IA embarqu√©e et le Computer Vision en temps r√©el

### Pourquoi embarquer l'IA sur des petits appareils ?

- **Confidentialit√©** : Pas besoin d'envoyer des donn√©es sensibles dans le cloud.
- **Autonomie** : Fonctionne sans connexion internet.
- **R√©duction des co√ªts** : Moins de d√©pendance √† une infrastructure serveur.

### Le d√©fi du Computer Vision en temps r√©el

D√©tecter un objet en quelques millisecondes avec des ressources limit√©es est un v√©ritable d√©fi. Voici les obstacles √† surmonter :

- Puissance de calcul limit√©e.
- Temps de traitement critique.
- Gestion des flux vid√©o en continu.
- Latence minimale pour une r√©ponse instantan√©e.

## Multiprocessing et IA Embarqu√©e

### Introduction

Bienvenue dans cette partie du workshop o√π nous allons parler d'optimisation et de **temps r√©el**. Dans notre cas, nous devons traiter un **flux vid√©o en continu**, ex√©cuter une **inf√©rence rapide**, et **envoyer une alerte** le plus vite possible.

Pour cela, nous allons exploiter **le multiprocessing** pour parall√©liser les t√¢ches et √©viter les goulets d'√©tranglement !

### Mat√©riel & Pr√©paration

Nous allons utiliser :

- **Raspberry Pi 5** (8 Go de RAM)
- **PiCamera Module 3** : MAX 30 FPS / 12Mpx
- **Batterie ou adaptateur secteur**

Maintenant que nous connaissons la Raspberry Pi, nous pouvons d√©couvrir tout son √©cosyst√®me, avec notamment, sa gamme de cam√©ras :

![camera](/assets/blog/welcome/camera.png)

L'un des sites principaux o√π trouver du mat√©riel :

![alt text](/assets/blog/welcome/kubii.png)

#### Challenges & Contraintes

Nous sommes dans un contexte **100% offline**, il faut donc **maximiser les performances** de notre syst√®me embarqu√© en jouant sur :

- **L'optimisation CPU & RAM**
- **Le multiprocessing** pour ex√©cuter plusieurs t√¢ches en parall√®le
- **Le choix d'une bonne r√©solution vid√©o** pour ne pas surcharger le traitement
- **La r√©duction de latence** pour un temps de r√©ponse optimal

### D√©roulement du TP

Nous allons concevoir un programme **divis√© en plusieurs modules**, chacun fonctionnant en **parall√®le** gr√¢ce au multiprocessing.

#### √âtape 1 : Cr√©ation des Modules

1. **Acquisition de la vid√©o**: Capturer le flux de la cam√©ra en continu.
2. **Inf√©rence avec YOLO**: D√©tecter une **intrusion humaine** en utilisant un mod√®le [**YOLO**](https://docs.ultralytics.com/fr).
3. **Alerte en cas d'intrusion**: Si une personne est d√©tect√©e, afficher un message.
4. **Affichage en temps r√©el**: Montrer la vid√©o avec les **bounding boxes** des intrus.

#### √âtape 2 : Installation de l'environnement de travail

- **Cr√©ation d'un environnement virtuel** afin d'isoler les d√©pendances :

Dans votre terminal :

```bash
python -m venv ENV_NAME
```

- **Installer les librairies** requises (`opencv`, `ultralytics`, `picamera2`)

Dans votre venv :

```bash
pip install -r requirements.txt
```

Voici le contenu du fichier `requirements.txt` :

```plaintext
ultralytics
opencv-python
websockets
picamera2
fastapi
uvicorn
streamlit
```

#### √âtape 3 : Multiprocessing en Action

Nous allons ex√©cuter chaque module dans un **processus s√©par√©** afin d'optimiser le traitement du flux vid√©o en **temps r√©el** !

![multiproc architecture](/assets/blog/welcome/multiproc.png)

Voici une structure de code possible :

```python
import multiprocessing
import cv2
from ultralytics import YOLO
from picamera2 import Picamera2
from multiprocessing import Queue
import numpy as np

def acquisition(queue: Queue) -> None:
    """
    Capture des images depuis la cam√©ra Raspberry Pi et les place dans une file d'attente.

    Args:
        queue (Queue): File d'attente pour stocker les images captur√©es.
    """
    picam2 = Picamera2()
    picam2.configure(picam2.create_preview_configuration(main={"size": (640, 640)}))
    picam2.start()

    while True:
        frame = picam2.capture_array()
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        queue.put(frame)

def inference(frame_queue: Queue, alert_queue: Queue, display_queue: Queue) -> None:
    """
    Effectue l'inf√©rence avec YOLOv8 pour d√©tecter les objets et envoie les r√©sultats.

    Args:
        frame_queue (Queue): File d'attente contenant les images √† traiter.
        alert_queue (Queue): File d'attente pour envoyer des alertes si une personne est d√©tect√©e.
        display_queue (Queue): File d'attente pour afficher les r√©sultats avec annotations.
    """
    model = YOLO("yolov8n.pt")  # Charge le mod√®le YOLOv8
    while True:
        frame = frame_queue.get()  # R√©cup√®re une image de la file d'attente
        results = model(frame)  # D√©tecte les objets

        # Cr√©er une copie de l'image pour ne montrer que les personnes
        frame_persons = frame.copy()

        for result in results:
            for box in result.boxes:
                cls = int(box.cls[0])  # Classe de l'objet d√©tect√©
                if cls == 0:  # Classe 0 = humain
                    alert_queue.put("Personne d√©tect√©e !")
                    # Dessiner seulement les bounding boxes des personnes
                    x1, y1, x2, y2 = map(int, box.xyxy[0])  # Coordonn√©es de la bo√Æte
                    cv2.rectangle(frame_persons, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame_persons, "Personne", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        display_queue.put(frame_persons)

def display_results(display_queue: Queue) -> None:
    """
    Affiche les r√©sultats de la d√©tection en temps r√©el.

    Args:
        display_queue (Queue): File d'attente contenant les images annot√©es √† afficher.
    """
    cv2.namedWindow("D√©tection de personnes", cv2.WINDOW_NORMAL)

    while True:
        frame = display_queue.get()
        cv2.imshow("D√©tection de personnes", frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cv2.destroyAllWindows()

def alert_system(alert_queue: Queue) -> None:
    """
    G√®re les alertes lorsqu'une personne est d√©tect√©e.

    Args:
        alert_queue (Queue): File d'attente contenant les messages d'alerte.
    """
    while True:
        message = alert_queue.get()
        print(f"WARNING {message}")

if __name__ == "__main__":
    frame_queue: Queue = multiprocessing.Queue()
    alert_queue: Queue = multiprocessing.Queue()
    display_queue: Queue = multiprocessing.Queue()

    p1 = multiprocessing.Process(target=acquisition, args=(frame_queue,))
    p2 = multiprocessing.Process(target=inference, args=(frame_queue, alert_queue, display_queue))
    p3 = multiprocessing.Process(target=alert_system, args=(alert_queue,))
    p4 = multiprocessing.Process(target=display_results, args=(display_queue,))

    p1.start()
    p2.start()
    p3.start()
    p4.start()

    p1.join()
    p2.join()
    p3.join()
    p4.join()
```

### R√©sultats Attendus

- La cam√©ra capture un flux vid√©o **en continu**
- L'inf√©rence YOLO d√©tecte une **pr√©sence humaine**
- Une alerte est envoy√©e d√®s qu'une **intrusion** est d√©tect√©e
- Le tout fonctionne **en parall√®le et en temps r√©el**

### Conclusion & Am√©liorations

Bravo ! Vous avez mis en place un syst√®me de **surveillance en temps r√©el** sur un **appareil embarqu√©** !

**Am√©liorations possibles :**

- **Optimiser la gestion de la m√©moire** pour √©viter les fuites.
- **Redimensionner les images** pour r√©duire la charge de traitement.
- **Appliquer la quantization** pour acc√©l√©rer l'inf√©rence.
- **Ajouter un mode serveur** pour envoyer les alertes sur un r√©seau local.

### BONUS : Qu'est-ce que la Quantization en IA ?

La quantization est une technique utilis√©e en intelligence artificielle, notamment pour optimiser les mod√®les de deep learning afin qu'ils fonctionnent plus efficacement sur des appareils √† ressources limit√©es.

Un mod√®le de deep learning utilise :

- poids
- des activations

Tous repr√©sent√©s en float32 (32 bits).

La quantization r√©duit la pr√©cision de ces valeurs en les convertissant en int8 (entiers sur 8 bits), ce qui diminue la taille du mod√®le et acc√©l√®re son ex√©cution, tout en conservant (souvent/parfois) des performances acceptables.

Finalement, nous obtenons un mod√®le plus l√©ger √† la fois √† charger dans la RAM mais aussi √† inf√©rer.

Puisque nous utilisons YOLO depuis la [biblioth√®que ultralytics](https://docs.ultralytics.com/fr/integrations/ncnn/#installation), allons voir comment cela s'applique √† notre cas :

```python
from ultralytics import YOLO

# Load the YOLO11 model
model = YOLO("yolo11n.pt")

# Export the model to NCNN format
model.export(format="ncnn")  # creates '/yolo11n_ncnn_model'

# Load the exported NCNN model
ncnn_model = YOLO("./yolo11n_ncnn_model")

# Run inference
results = ncnn_model("https://ultralytics.com/images/bus.jpg")
```

Tentez de nouveau d'√©x√©cuter ce programme en chargeant notre nouveau r√©seau !

## Conclusion

Vous avez maintenant toutes les bases pour d√©ployer de l'IA embarqu√©e sur Raspberry Pi et d'autres appareils !
